{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 3: Preparación y Modelado con Regresión Logística\n",
    "\n",
    "En este proyecto se realizó la preparación de datos y el modelado utilizando un modelo de regresión logística aplicado al conjunto de datos Skin Segmentation. Se evaluaron los resultados utilizando dos esquemas de validación: N-folds validation con N=10 y random subsampling con una partición 70/30, utilizando 10 repeticiones. Los resultados se evaluaron en términos de matriz de confusión, precisión, sensibilidad (recall), especificidad, precisión y F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Bibliotecas Necesarias\n",
    "Se importaron las bibliotecas para la preprocesamiento de datos, la creación del modelo de regresión logística y la evaluación de su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa las bibliotecas necesarias\n",
    "import numpy as np  # Se utilizaron para operaciones numéricas\n",
    "import pandas as pd  # Se utilizaron para manipulación de datos en estructuras de DataFrame\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_validate  # Se utilizaron para dividir datos y validar modelos\n",
    "from sklearn.linear_model import LogisticRegression  # Se utilizó para crear el modelo de regresión logística\n",
    "from sklearn.preprocessing import MinMaxScaler  # Se utilizó para normalizar los datos\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score  # Se utilizaron para evaluar el rendimiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga y Preparación de los Datos\n",
    "Ene sta sección, se cargaron los datos, se separaron las características junto con la variable objetivo, y se normalizaron las características utilizando Min-Max Scaling.\n",
    "\n",
    "Los siguientes datos se obtuvieron de la página del dataset usado (**Skin Segmentationn**): \n",
    "- El conjunto de datos usado contiene valores de color BGR (Blue, Green, Red de **tipo entero**) y una variable llamada \"y\" de tipo **binario** que determina si el pixel es de piel o no .\n",
    "- Fueron obtenidos desde imágenes faciales de personas de diferentes edades, géneros y razas. \n",
    "- Incluye un total de 245,057 muestras, de las cuales **50,859 son de piel y 194,198 son de no piel**. \n",
    "- **No tiene valores faltantes** y se utiliza principalmente para tareas de clasificación, donde el objetivo es diferenciar entre píxeles de piel y no piel en una imagen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Carga del Dataset\n",
    "Se cargó el conjunto de datos Skin Segmentation desde el archivo proporcionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>G</th>\n",
       "      <th>R</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>85</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>84</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>83</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>81</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>81</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    B   G    R  y\n",
       "0  74  85  123  1\n",
       "1  73  84  122  1\n",
       "2  72  83  121  1\n",
       "3  70  81  119  1\n",
       "4  70  81  119  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga los datos\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt'\n",
    "column_names = ['B', 'G', 'R', 'y']\n",
    "data = pd.read_csv(url, delim_whitespace=True, header=None, names=column_names)\n",
    "\n",
    "# Muestra las primeras filas del dataset\n",
    "data.head()"
   ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 3. Preparación del Dataset\n",
     "En esta sección, se realizó la preparación del dataset, incluyendo el balanceo de clases mediante subsampling, la división en conjuntos de entrenamiento y prueba, y la normalización de los datos."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### 3.1. Balanceo de Clases por Subsampling\n",
     "Dado que el conjunto de datos Skin Segmentation presenta un desbalance de clases, donde la clase de 'no piel' es significativamente mayor que la clase de 'piel', se aplicó un balanceo de clases mediante subsampling. El subsampling se utilizó porque el número total de muestras es grande (245,057 muestras), lo que nos permite entrenar el modelo con más de 100,000 muestras equilibradas después del subsampling."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Balanceo de clases por subsampling\n",
     "from sklearn.utils import resample\n",
     "import matplotlib.pyplot as plt\n",
     "\n",
     "# Combina X e y para facilitar el resampling\n",
     "data_combined = pd.concat([pd.DataFrame(X, columns=['B', 'G', 'R']), pd.Series(y, name='y')], axis=1)\n",
     "\n",
     "# Verifica la distribución de clases antes del balanceo\n",
     "class_counts_before = data_combined['y'].value_counts()\n",
     "plt.figure(figsize=(8, 6))\n",
     "class_counts_before.plot(kind='bar', color=['red', 'blue'])\n",
     "plt.title('Distribución de Clases Antes del Balanceo')\n",
     "plt.xlabel('Clase')\n",
     "plt.ylabel('Número de muestras')\n",
     "plt.show()\n",
     "\n",
     "# Separa las clases\n",
     "skin = data_combined[data_combined['y'] == 1]\n",
     "nonskin = data_combined[data_combined['y'] == 2]\n",
     "\n",
     "# Realiza subsampling en la clase mayoritaria (nonskin)\n",
     "nonskin_downsampled = resample(nonskin,\n",
     "                              replace=False,    # No reemplaza, realiza subsampling\n",
     "                              n_samples=len(skin),  # Submuestrea al tamaño de la clase minoritaria (skin)\n",
     "                              random_state=42)  # Asegura la reproducibilidad\n",
     "\n",
     "# Combina las clases balanceadas\n",
     "balanced_data = pd.concat([skin, nonskin_downsampled])\n",
     "\n",
     "# Mezcla los datos balanceados\n",
     "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
     "\n",
     "# Verifica la distribución de clases después del balanceo\n",
     "class_counts_after = balanced_data['y'].value_counts()\n",
     "plt.figure(figsize=(8, 6))\n",
     "class_counts_after.plot(kind='bar', color=['red', 'blue'])\n",
     "plt.title('Distribución de Clases Después del Balanceo')\n",
     "plt.xlabel('Clase')\n",
     "plt.ylabel('Número de muestras')\n",
     "plt.show()"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### 3.2. División en Conjunto de Entrenamiento y Prueba\n",
     "Después de balancear las clases, se dividió el dataset en un conjunto de entrenamiento (70%) y un conjunto de prueba (30%) con estratificación basada en la variable objetivo. Esto asegura que la proporción de clases en la variable objetivo se mantenga consistente entre ambos conjuntos."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Divide el dataset balanceado en conjuntos de entrenamiento y prueba\n",
     "X = balanced_data[['B', 'G', 'R']].values\n",
     "y = balanced_data['y'].values\n",
     "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### 3.3. Normalización de los Datos\n",
     "En esta subsección, se realizó la normalización de las características utilizando Min-Max Scaling. Antes de aplicar la normalización, se generaron gráficos de las distribuciones de las componentes B, G y R para analizar su comportamiento y justificar la elección de la técnica de normalización."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "#### 3.3.1. Análisis de Distribuciones y Elección de Min-Max Scaling\n",
     "A continuación se muestran las distribuciones de las componentes B, G y R del conjunto de datos de entrenamiento balanceado. Como se observa, las distribuciones no son gaussianas, lo que hace que la normalización Min-Max sea una opción más adecuada que la normalización z-score. Dado que los valores de B, G y R son positivos, se decidió utilizar la normalización en el rango [0, 1], lo cual es consistente con buenas prácticas en el preprocesamiento de datos, especialmente cuando se usan modelos de machine learning que pueden incluir capas con funciones de activación ReLU, evitando la pérdida de información que podría ocurrir si se introducen valores negativos en estas funciones."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Importa las bibliotecas necesarias para la visualización\n",
     "import seaborn as sns\n",
     "\n",
     "# Configura el estilo de los gráficos\n",
     "sns.set(style=\"whitegrid\")\n",
     "\n",
     "# Genera los gráficos de las distribuciones de las componentes B, G y R\n",
     "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
     "sns.histplot(X_train[:, 0], bins=50, kde=True, ax=axes[0], color='blue').set_title('Distribución de B (Blue)')\n",
     "sns.histplot(X_train[:, 1], bins=50, kde=True, ax=axes[1], color='green').set_title('Distribución de G (Green)')\n",
     "sns.histplot(X_train[:, 2], bins=50, kde=True, ax=axes[2], color='red').set_title('Distribución de R (Red)')\n",
     "plt.show()"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "#### 3.3.2. Aplicación de Min-Max Scaling\n",
     "Se aplicó Min-Max Scaling a las características del conjunto de datos de entrenamiento balanceado. El escalador ajustado en el conjunto de entrenamiento se utilizó para transformar tanto el conjunto de entrenamiento como el conjunto de prueba, asegurando así que la normalización se realizó de manera consistente."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Aplica Min-Max Scaling al conjunto de entrenamiento balanceado\n",
     "scaler = MinMaxScaler()\n",
     "X_train = scaler.fit_transform(X_train)\n",
     "\n",
     "# Aplica la transformación al conjunto de prueba\n",
     "X_test = scaler.transform(X_test)\n",
     "\n",
     "# Muestra las primeras filas de las características normalizadas\n",
     "pd.DataFrame(X_train, columns=['B', 'G', 'R']).head()"
    ]
   },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creación y Ajuste del Modelo de Regresión Logística\n",
    "En esta sección, se creó y ajustó un modelo de regresión logística utilizando los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Creación del Modelo\n",
    "Se creó un modelo de regresión logística con un máximo de 1000 iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el modelo de regresión logística\n",
    "model = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Ajuste del Modelo con los Datos de Entrenamiento\n",
    "Se ajustó el modelo utilizando el conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta el modelo con los datos de entrenamiento\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluación del Modelo\n",
    "En esta sección, se evaluó el modelo utilizando dos métodos: validación cruzada con N-folds (N=10) y random subsampling con 10 repeticiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Validación Cruzada con N-Folds (N=10)\n",
    "Se realizó una validación cruzada utilizando 10 pliegues para evaluar la precisión y otras métricas del modelo en todo el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura la validación cruzada con 10 pliegues\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Evalúa el modelo con la validación cruzada y calcula las métricas requeridas\n",
    "conf_matrices = []\n",
    "specificities = []\n",
    "\n",
    "for train_idx, test_idx in cv.split(X, y):\n",
    "    X_train_cv, X_test_cv = X[train_idx], X[test_idx]\n",
    "    y_train_cv, y_test_cv = y[train_idx], y[test_idx]\n",
    "    model.fit(X_train_cv, y_train_cv)\n",
    "    y_pred_cv = model.predict(X_test_cv)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test_cv, y_pred_cv)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    specificities.append(tn / (tn + fp))\n",
    "\n",
    "# Calcula la matriz de confusión promedio\n",
    "mean_conf_matrix_cv = np.mean(conf_matrices, axis=0)\n",
    "mean_specificity_cv = np.mean(specificities)\n",
    "\n",
    "# Muestra los resultados\n",
    "print('Matriz de confusión promedio (Validación Cruzada):\\n', mean_conf_matrix_cv)\n",
    "print(f'Specificity promedio: {mean_specificity_cv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Evaluación utilizando Random Subsampling (70/30) con 10 Repeticiones\n",
    "Se realizó una evaluación del modelo utilizando la técnica de random subsampling con 10 repeticiones, evaluando las métricas de precisión, sensibilidad, especificidad y F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza random subsampling con 10 repeticiones\n",
    "random_state = 42\n",
    "accuracy_list, precision_list, recall_list, f1_list, specificity_list = [], [], [], [], []\n",
    "conf_matrices = []\n",
    "\n",
    "for i in range(10):\n",
    "    X_train_rs, X_test_rs, y_train_rs, y_test_rs = train_test_split(X, y, test_size=0.3, random_state=random_state, stratify=y)\n",
    "    model.fit(X_train_rs, y_train_rs)\n",
    "    y_pred_rs = model.predict(X_test_rs)\n",
    "    \n",
    "    # Calcula las métricas\n",
    "    accuracy_list.append(accuracy_score(y_test_rs, y_pred_rs))\n",
    "    precision_list.append(precision_score(y_test_rs, y_pred_rs))\n",
    "    recall_list.append(recall_score(y_test_rs, y_pred_rs))\n",
    "    f1_list.append(f1_score(y_test_rs, y_pred_rs))\n",
    "    conf_matrix = confusion_matrix(y_test_rs, y_pred_rs)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    specificity_list.append(specificity)\n",
    "    \n",
    "    random_state += 1\n",
    "\n",
    "# Calcula la matriz de confusión promedio\n",
    "mean_conf_matrix_rs = np.mean(conf_matrices, axis=0)\n",
    "\n",
    "# Muestra la matriz de confusión promedio y las métricas\n",
    "print('Matriz de confusión promedio (Random Subsampling):\\n', mean_conf_matrix_rs)\n",
    "print(f'Accuracy promedio: {np.mean(accuracy_list)}')\n",
    "print(f'Precision promedio: {np.mean(precision_list)}')\n",
    "print(f'Recall promedio: {np.mean(recall_list)}')\n",
    "print(f'F1-score promedio: {np.mean(f1_list)}')\n",
    "print(f'Specificity promedio: {np.mean(specificity_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análisis de Resultados\n",
    "En esta sección, se comentaron los resultados obtenidos en las evaluaciones, comparando los dos esquemas de validación y discutiendo cuál es más apropiado para este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Comparación entre Validación Cruzada y Random Subsampling\n",
    "En esta actividad, se compararon dos esquemas de validación diferentes: validación cruzada con 10 pliegues y random subsampling con 10 repeticiones.\n",
    "\n",
    "La validación cruzada es útil para evaluar la robustez del modelo en diferentes subconjuntos de datos, mientras que el random subsampling proporciona una forma de evaluar el modelo en particiones aleatorias específicas.\n",
    "\n",
    "En este caso, ambos esquemas proporcionaron resultados similares en términos de las métricas evaluadas, lo que sugiere que el modelo de regresión logística es consistente en su rendimiento. Sin embargo, la validación cruzada puede ser preferible en escenarios donde se desea evaluar la estabilidad del modelo en diferentes subconjuntos del dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
